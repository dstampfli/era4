{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU IProgress\n",
    "%pip install -qU ipywidgets\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU tqdm\n",
    "\n",
    "%pip install -qU langchain-core\n",
    "%pip install -qU langchain-community\n",
    "%pip install -qU langchain-openai \n",
    "%pip install -qU langchain-qdrant \n",
    "\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs from a directory\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def process_directory(path: str, glob: str, loader_cls: str, use_multithreading=True):\n",
    "\t\n",
    "\tloader = DirectoryLoader(path=path, glob=glob, show_progress=True, loader_cls=loader_cls, use_multithreading=use_multithreading)\n",
    "\t\n",
    "\tdocs = loader.load()\n",
    "\t\n",
    "\treturn docs\n",
    "\n",
    "#####\n",
    "\n",
    "def test_process_directory():\n",
    "\tdocs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\tprint(len(docs))\n",
    "\n",
    "# test_process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def create_embeddings_openai(model=\"text-embedding-ada-002\") -> OpenAIEmbeddings:\n",
    "\n",
    "\t# Initialize the OpenAIEmbeddings class\n",
    "\tembeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "\treturn embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_openai():\n",
    "\ttext = \"What is the annual revenue of Uber?\"\n",
    "\t\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector = embeddings.embed_query(text)\n",
    "\t\n",
    "\tprint(vector)\n",
    "\n",
    "# test_create_embeddings_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using recursive character text splitter\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs_recursive(docs: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\tchunks = text_splitter.split_documents(docs)\n",
    "\n",
    "\treturn chunks\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_recursive(): \n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\t\n",
    "\tprint(f\"\\nNumber of chunks = {len(chunks)}\\n\")\n",
    "\tprint(f\"First chunk = {chunks[0].page_content}\")\n",
    "\n",
    "# test_chunk_docs_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant vector store\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_qdrant_vector_store(location: str, \n",
    "\t\t\t\t\t\t\t   collection_name: str, \n",
    "\t\t\t\t\t\t\t   vector_size: int, \n",
    "\t\t\t\t\t\t\t   embeddings: Embeddings, \n",
    "\t\t\t\t\t\t\t   docs: list) -> QdrantVectorStore:\n",
    "\n",
    "\t# Initialize the Qdrant client\n",
    "\tqdrant_client = QdrantClient(location=location)\n",
    "\n",
    "\t# Create a collection in Qdrant\n",
    "\tqdrant_client.create_collection(collection_name=collection_name, \n",
    "\t\t\t\t\t\t\t\t vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "\n",
    "\t# Initialize QdrantVectorStore with the Qdrant client\n",
    "\tqdrant_vector_store = QdrantVectorStore(client=qdrant_client, \n",
    "\t\t\t\t\t\t\t\t\t\t collection_name=collection_name, embedding=embeddings)\n",
    "\t\n",
    "\t# Add the docs to the vector store\n",
    "\tqdrant_vector_store.add_documents(docs)\n",
    "\t\n",
    "\treturn qdrant_vector_store\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_qdrant_vector_store():\n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\t\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\t\n",
    "\tprint(vector_store.collection_name)\n",
    "\n",
    "# test_create_qdrant_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant retriever\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def create_retriever_qdrant(vector_store: QdrantVectorStore) -> BaseRetriever:\n",
    "\n",
    "\tretriever = vector_store.as_retriever()\n",
    "\n",
    "\treturn retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_qdrant(text):\n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\t\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\t\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\t\n",
    "\tdocs = retriever.invoke(text)\t\n",
    "\tprint(docs[0])\n",
    "\n",
    "# test_create_retriever_qdrant(\"What is the annual revenue for Uber?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#chatprompttemplate\n",
    "# https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_chat_prompt_template(template: str = None) -> ChatPromptTemplate:\n",
    "\t\n",
    "\tif template is None:\n",
    "\t\ttemplate = '''\n",
    "\t\tYou are an expert assistant designed to help users analyze and answer questions about 10K annual reports filed by publicly traded companies. Users may ask about specific sections, financial metrics, trends, or comparisons across multiple reports. Your role is to provide accurate, concise, and relevant answers, referencing appropriate sections or data points where applicable.\n",
    "\n",
    "\t\tWhen responding, adhere to the following principles:\n",
    "\n",
    "\t\tUnderstand the Question: Identify whether the query is focused on a specific company, year, or metric, or if it spans multiple reports for comparison.\n",
    "\t\tClarify Uncertainty: If a user's question is unclear, ask for clarification or additional context.\n",
    "\t\tLocate and Reference Information: Use relevant sections of the 10K report(s), such as MD&A, Financial Statements, Risk Factors, or Notes to Financial Statements, to back up your answers.\n",
    "\t\tSynthesize Data: Provide summaries or insights when the question involves comparing data or trends across multiple reports.\n",
    "\t\tStay Objective: Avoid providing subjective opinions or interpretations beyond the factual content in the reports.\n",
    "\t\tExample User Queries and Expected Responses:\n",
    "\n",
    "\t\t\"What was the revenue for Company X in 2022 and 2023?\"\n",
    "\n",
    "\t\tLocate and report the revenue figures from the Income Statements of the respective 10K reports for 2022 and 2023.\n",
    "\t\t\"What are the main risk factors for Company Y in its latest report?\"\n",
    "\n",
    "\t\tSummarize the key risk factors from the most recent 10K report's \"Risk Factors\" section.\n",
    "\t\t\"How did the operating income of Company Z change over the last three years?\"\n",
    "\n",
    "\t\tExtract operating income figures from the 10K reports for the past three years and provide a brief comparison.\n",
    "\t\t\"Compare the debt levels of Company A and Company B in 2023.\"\n",
    "\n",
    "\t\tRetrieve debt-related figures from the Balance Sheets or Notes to Financial Statements of both companies and summarize the comparison.\n",
    "\t\t\"What trends are evident in Company W's R&D expenses over the last five years?\"\n",
    "\n",
    "\t\tSummarize trends using data from the Income Statements or footnotes for R&D expenses across five consecutive 10K reports.\n",
    "\t\tAssumptions and Constraints:\n",
    "\n",
    "\t\tOnly use inforomation from 10K reports provided in the context below. \n",
    "\t\tFor complex queries spanning multiple reports, provide a structured summary highlighting key comparisons or trends.\n",
    "\t\tIf certain information is unavailable, state so clearly and suggest alternative approaches to obtain it.\n",
    "\n",
    "\t\tNow it's your turn!\n",
    "\t\t\n",
    "\t\t{question}\n",
    "\n",
    "\t\t{context}\n",
    "\t\t'''\n",
    "\t\n",
    "\tprompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\treturn prompt\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chat_prompt_template():\n",
    "\tprompt = create_chat_prompt_template()\n",
    "\t\n",
    "\tprint(prompt)\n",
    "\n",
    "# test_create_chat_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Langchain chain\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSerializable\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def create_chain (model: str, \n",
    "\t\t\t\t  prompt_template: ChatPromptTemplate, \n",
    "\t\t\t\t  retriever: BaseRetriever\n",
    "\t\t\t\t  ) -> RunnableSerializable:\n",
    "\n",
    "\tllm = ChatOpenAI(model=model)\n",
    "\t\t\n",
    "\tchain = (\n",
    "\t\t{\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "\t\t| RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "\t\t| {\"response\": prompt_template | llm, \"context\": itemgetter(\"context\")}\n",
    "\t\t)\n",
    "\n",
    "\treturn chain\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chain_qdrant():\n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain(\"gpt-4o-mini\", chat_prompt_template, retriever)\n",
    "\t\n",
    "\tresult = chain.invoke({\"question\" : \"What is the annual revenue of Uber?\"})\n",
    "\tprint(result)\n",
    "\n",
    "# test_create_chain_qdrant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers from a chain using a list of questions\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "\n",
    "\n",
    "def generate_answers_contexts(chain: RunnableSerializable, \n",
    "\t\t\t\t\t\t\t  questions: list\n",
    "\t\t\t\t\t\t\t  ) -> Tuple[List, List]:\n",
    "\t\n",
    "\tanswers = []\n",
    "\tcontexts = []\n",
    "\n",
    "\t# Loop over the list of questions and call the chain to get the answer and context\n",
    "\tfor question in questions:\n",
    "\t\tprint(question)\n",
    "\n",
    "\t\t# Call the chain to get answers and contexts\n",
    "\t\tresponse = chain.invoke({\"question\" : question})\n",
    "\t\tprint(response)\n",
    "\n",
    "\t\t# Capture the answer and context \n",
    "\t\tanswers.append(response[\"response\"].content)\n",
    "\t\tcontexts.append([context.page_content for context in response[\"context\"]])\n",
    "\n",
    "\treturn answers, contexts\n",
    "\n",
    "#####\n",
    "\n",
    "def test_generate_answers_contexts():\n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\t\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain(\"gpt-4o-mini\", chat_prompt_template, retriever)\n",
    "\t\n",
    "\tquestions = [\"What is the annual revenue of Lyft?\",\n",
    "\t\t\t  \"What is the annual revenue of Uber?\",\n",
    "\t\t\t  \"Which company has a larger annual revenue - Lyft or Uber?\"]\t\n",
    "\t\n",
    "\tanswers, contexts = generate_answers_contexts(chain=chain, questions=questions)\n",
    "\t\n",
    "\tprint(f\"Total number of answers = {len(answers)}\")\n",
    "\tprint(f\"Total number of contexts = {len(contexts)}\")\n",
    "\n",
    "# test_generate_answers_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Ragas evaluation \n",
    "\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "def run_ragas_evaluation(chain: RunnableSerializable, \n",
    "\t\t\t\t\t\tquestions: list, \n",
    "\t\t\t\t\t\tgroundtruths: list, \n",
    "\t\t\t\t\t\teval_metrics: list = [answer_correctness, answer_relevancy, context_recall, context_precision, faithfulness]\n",
    "\t\t\t\t\t\t):\n",
    "\t\n",
    "\tanswers = []\n",
    "\tcontexts = []\n",
    "\tanswers, contexts = generate_answers_contexts(chain=chain, questions=questions)\n",
    "\n",
    "\t# Create the input dataset \n",
    "\tinput_dataset = Dataset.from_dict({\"question\" : questions,       \t# From the dataframe\n",
    "\t\t\t\t\t\t\t\t\t\t\"answer\" : answers,             # From the chain\n",
    "\t\t\t\t\t\t\t\t\t\t\"contexts\" : contexts,          # From the chain\n",
    "\t\t\t\t\t\t\t\t\t\t\"ground_truth\" : groundtruths   # From the dataframe\n",
    "\t\t\t\t\t\t\t\t\t\t})\n",
    "\n",
    "\t# Run the Ragas evaluation using the input dataset and eval metrics\n",
    "\tragas_results = evaluate(input_dataset, eval_metrics)\n",
    "\tragas_results_df = ragas_results.to_pandas()\n",
    "\t\n",
    "\treturn ragas_results, ragas_results_df\n",
    "\t\n",
    "#####\n",
    "\n",
    "def test_run_ragas_evaluation():\n",
    "\tdocs = process_directory(\"docs/\", \"**/*.pdf\", PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\t\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\t\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain(\"gpt-4o-mini\", chat_prompt_template, retriever)\n",
    "\t\n",
    "\ttestset_df = pd.read_csv(\"testsets/10k_test_testset.csv\")\t# This testset contains a small number of questions\n",
    "\tquestions = testset_df[\"question\"].values.tolist()\n",
    "\tquestions = [str(question) for question in questions]\n",
    "\tgroundtruths = testset_df[\"ground_truth\"].values.tolist()\n",
    "\tgroundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\teval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\tragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\t\n",
    "\ttimestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\tragas_results_df.to_csv(f\"evaluations/10x_test_testset_evaluation_{timestr}.csv\")\n",
    "\t\n",
    "\tprint(ragas_results)\n",
    "\n",
    "# test_run_ragas_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1a - OpenAI and Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chain using OpenAI and Qdrant\n",
    "\n",
    "embeddings = create_embeddings_openai()\n",
    "docs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "chunks = chunk_docs_recursive(docs=docs)\n",
    "print(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "vector_store = create_qdrant_vector_store(':memory:', 'holiday-test', 1536, embeddings, chunks)\n",
    "retriever = create_retriever_qdrant(vector_store)\n",
    "chat_prompt_template = create_chat_prompt_template()\n",
    "chain = create_chain('gpt-4o', chat_prompt_template, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain with a few questions \n",
    "\n",
    "questions = [\"What is the annual revenue of Uber?\",\n",
    "\"What is the annual revenue of Lyft?\",\n",
    "\"How does Uber's revenue compare to Lyft's revenue?\",]\n",
    "\n",
    "for question in questions:\n",
    "\tprint(question)\n",
    "\tresult = chain.invoke({\"question\" : question})\n",
    "\tprint(result)\n",
    "\tprint(result[\"response\"].content)\n",
    "\tprint(\"\\n*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the chain using Ragas\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "# Get the questions and groundtruths from the dataframe\n",
    "testset_df = pd.read_csv(\"testsets/10k_full_testset.csv\")\n",
    "\n",
    "questions = testset_df[\"question\"].values.tolist()\n",
    "questions = [str(question) for question in questions]\n",
    "\n",
    "groundtruths = testset_df[\"ground_truth\"].values.tolist()\n",
    "groundtruths = [str(ground_truth) for ground_truth in groundtruths]  \n",
    "\n",
    "# Specify the eval metrics\n",
    "eval_metrics = [answer_correctness, answer_relevancy, context_precision, context_recall, faithfulness]\n",
    "\n",
    "# Run the Ragas evaluation and show the results\n",
    "ragas_results, ragas_results_df = run_ragas_evaluation(chain, questions, groundtruths, eval_metrics)\n",
    "\n",
    "# Write the results to disk\n",
    "timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "ragas_results_df.to_csv(f\"evaluations/10x_test1_testset_evaluation_{timestr}.csv\")\n",
    "\n",
    "# Show the resutls\n",
    "print(ragas_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "era4-holiday-test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
